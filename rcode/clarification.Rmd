##  DATA CLARIFICATION
##
##  Input: cpu-clean.csv
##  Output: None
##
##
##  Description:
##
##  Caveats:
##
##  NK
##  ---









PART I. OVERVIEW OF THE FEATURES

LOADING PACKAGES & IMPORTING DATA
```{r}
pacman::p_load(
  rio,     # for imports & exports
  ggplot2, # for plots
  zoo      # for year-quarter formats
)
##  IMPORT THE DATA
setwd("/Users/admin/Desktop/_probability_PROJECT/btlprob/rcode")   # set working directory
data <- import("cpu-clean.csv")        # rio::import
```

CATEGORIAL PIES
```{r}
# Config market pie
freq = as.vector(table(data$market))
levels = levels(factor(data$market))
title = "market"
```

```{r}
# Config status pie
freq = as.vector(table(data$status))
levels = levels(factor(data$status))
title = "status"
```

```{r}
# Drawing the pie
# Calculate the vector of percentages
percentages <- round(
  100 * freq / sum(freq), 
  1
  )

# Create the data frame of levels, frequency of values, and percentages
data_percentages <- data.frame(
  market = levels, 
  value = freq, 
  percent = percentages
  )

# Create a pie chart with percentages
ggplot(
  data_percentages, 
  aes(x="", y=value, fill=market)) +
  geom_bar(stat="identity", width=1, color="white") + # Draw the bar
  coord_polar(theta = "y") +               # Representing the values in polar
                                           # coordinate. Instead of stacked bars
  # Add labels (percentage numbers)
  geom_text(
    aes(label = paste0(percent, "%")),     # Concat 'percent' + '%'
    position = position_stack(vjust = 0.5) # Each slide is a stack bar
                                           # Use position_stack to stack the labels
                                           # on each slide.
    ) +
  labs(fill = title) +
  theme_void()
```

Histogram of launch date
```{r}
ggplot(data, aes(x = ldate)) +
  geom_histogram(binwidth = 1, fill="deepskyblue") +
  labs(x = "Date", y = "Count")
```

Boxplot of Lithography
```{r}
ggplot(data, aes(x = market, y = litho)) +
  geom_boxplot(fill="deepskyblue") +
  labs(x = "Market", y = "Lithography (nm)")

summary(data$litho)
```

Scatter plot of Lithography
```{r}
ggplot(data, aes(x = ldate, y = litho)) +
  geom_point(color="deepskyblue") +
  labs(x = "Launch Date", y = "Lithography (nm)")
```

Boxplot of Recommended Price
```{r}
ggplot(data, aes(x = market, y = rprice)) +
  geom_boxplot(fill="deepskyblue") +
  labs(x = "Market", y = "Recommended Price ($)")
```

Histogram of Base frequency
```{r}
ggplot(data, aes(x = bfreq)) +
  geom_histogram(binwidth = 0.1, fill="deepskyblue") +
  labs(x = "Base frequency (GHz)", y = "Count")
```

Histogram of Thermal Design Power
```{r}
ggplot(data, aes(x = market,y = tdp)) +
  geom_boxplot(fill="deepskyblue") +
  labs(x = "Market", y = "Thermal deisgn power (W)")

summary(data$tdp)
```

Histogram of Memmory Bandwith
Currently hidden in our report.
```{r}
ggplot(data, aes(y = memband)) +
  geom_boxplot(fill="deepskyblue") +
  labs(x = "Max Memory Bandwidth (GB/s)", y = "Count")

summary(data$memband)
```

Histogram of Temperature
```{r}
ggplot(data, aes(x = temp)) +
  geom_histogram(binwidth = 1, fill="deepskyblue") +
  labs(x = "Temperature (°C)", y = "Count")
```









PART 2. ANALYSIS OF FEATURES

[litho & ncore ~ bfreq]

? Why we are interested in Base frequency ?
bfreq is a very good representative for CPU's performance, so we will explore
how other factors contribute to the performance of the CPU.

? Why we are interested in litho ?
Lithography is represents well for the distinction of each period, in fact, it is
a better representation for `ldate`. Refer to `the plot` we provided in the last section,
with bare eyes, we can see `litho` is associated with the launch-date very well, and
it is decreasing over time. What makes it better than `ldate` is that `litho` spans
over a period of time, and not fixed to a specific year-quarter. One more advantage
is that, some records do not have launch dates, but they have lithography instead, so using
it is better to gain more data. We would like to see the impact of decreasing lithography
on the performance of the CPU.

? Why we are interested in ncore ?
Number of cores is the driving factor of modern computing, which helps computer utilizing 
the power of parallelization, which is considered to be a work-around for the approaching lower limit Lithography of 1.5 nanometers. [refer to (https://www.quora.com/Have-we-reached-the-limit-in-lithography-of-CPUs)]. On the other hand, more cores also means more cost for a CPU to be manufactured. Because of that, analyzing the number of cores with respect to its perform is neccessary to decide
whether it is worthy to invest a lot of money produce cores, and whether increasing no. cores impact negatively to the base frequency.

```{r}
# We plot the histogram of litho ~ bfreq
ggplot(data, aes(x = bfreq)) +
  geom_histogram(binwidth = 0.1, fill="deepskyblue") + 
  facet_wrap(~data$litho)

# We see that some categories of lithography are not significant in terms of
# quantity, and do not follow any clear distribution. We will treat them as
# outliners.

# We choose to keep the following category:
# 14, 22, 32, 45:
data <- subset(data, data$litho %in% c(14,22,32,45))

# We plot the histogram of ncore ~ bfreq
ggplot(data, aes(x = bfreq)) +
  geom_histogram(binwidth = 0.1, fill="deepskyblue") + 
  facet_wrap(~data$ncore)

# We see that some categories of ncore are not significant in terms of quantity
# -> remove it too

# We choose to keep the following category:
# 1, 2, 4, 6 ,8, 10, 12
data <- subset(data, data$ncore %in% c(2,4))

# Group them as factors
data$ncore <- as.factor(data$ncore)
data$litho <- as.factor(data$litho)

# Now, we visualize all of this up, to see what did we get so far:
ggplot(data, aes(x = ncore,y = bfreq,color = litho))+
  geom_boxplot()
```


Since the variance and normality is not perfect, we better do a non-parametric test,
such as Kruskal–Wallis one-way analysis of variance.
ANOVA - one way [litho ~ bfreq]

We can see that the values of the test is small (< 0.05), we can say that there are significant differences between the
"ranks" (<- explain this furter)
```{r}
ggplot(data, aes(x = litho,y = bfreq))+
  geom_boxplot(fill = "deepskyblue")
kruskal.test(bfreq ~ litho, data = data)
```


To see which group is significantly different from others, we must perform a
post-hoc comparison, in this case, we use Wilcoxon test
[https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance]
```{r}
pairwise.wilcox.test(data$bfreq, data$litho, p.adjust.method = "bonferroni")
```




ANOVA - one way (ncore ~ bfreq)

```{r}
# OK YOU GET THE IDEA, PLEASE DO THAT FOR ME VIETTUNG
```

ANOVA - two way [litho & ncore ~ bfreq]

Now we construct Two-way ANOVA model
```{r}
model<- aov(bfreq ~ litho * ncore ,data = data)
qqPlot(residuals(model))
shapiro.test(residuals(model))
leveneTest(bfreq ~ litho *  ncore ,data = data)
Anova(model, type='III')
```


Transform the data to rank-based. Nonparametric Two-way ANOVA
"Finaly, let’s perform two-way ANOVA on the rank-transformed data. Ranking is one of many procedures used to transform data that do not meet the assumptions of normality. Conover and Iman (1981) provided a review of the four main types of rank transformations. One method replaces each original data value by its rank (from 1 for the smallest to N for the largest, where N is the combined data sample size)"
[https://www.cfholbert.com/blog/nonparametric_two_way_anova/]

As we can see, it looks worse.
```{r}
model<- aov(rank(bfreq) ~ litho * ncore ,data = data)
qqPlot(residuals(model))
shapiro.test(residuals(model))
leveneTest(rank(bfreq) ~ litho *  ncore ,data = data)
Anova(model, type='III')
```

One frustrating thing about tests is that they are very sensitive to large data.
So do not trust in tests, but the intuition.















































